{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"iKD4yfO7k5Le"},"source":["# Classificação de sentimentos do Twitter"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Autor\n","\n","Samuel Nogueira Bacelar - 180130722\n","\n","Github: [SamuelNoB](https://github.com/SamuelNoB)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Objetivo\n","\n","O objetivo desse artigo é treinar um modelo capaz de identificar os textos de comentários do Twitter e classificá-los de acordo com o sentimento como positiva ou negativa.\n","\n","## Motivo\n","\n","A fim de aplicar conhecimentos de NLP uma das formas de utilizá-lo é a partir da classificação textual. Além disso, os comentários e postagens (tweets) em redes sociais como o twitter podem necessitar de uma classificação prévia e simples de modo a facilitar a identificação de publicações nocivas e que podem vir a ferir as regras da plataforma."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Instalação de depêndencias"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-12-14T14:51:19.157789Z","iopub.status.busy":"2022-12-14T14:51:19.156894Z","iopub.status.idle":"2022-12-14T14:51:32.692636Z","shell.execute_reply":"2022-12-14T14:51:32.691281Z","shell.execute_reply.started":"2022-12-14T14:51:19.157694Z"},"id":"DX2LI2_6k5Lb","outputId":"cdc6b3d4-9137-4d39-af55-18dc7a7d6f8f","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (4.28.0)\n","Requirement already satisfied: datasets in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (2.12.0)\n","Requirement already satisfied: evaluate in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (0.4.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from transformers) (0.13.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from transformers) (2023.5.5)\n","Requirement already satisfied: requests in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from transformers) (2.28.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from transformers) (23.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from transformers) (0.14.1)\n","Requirement already satisfied: filelock in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from transformers) (3.10.7)\n","Requirement already satisfied: numpy>=1.17 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from transformers) (1.24.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from transformers) (4.65.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from transformers) (6.0)\n","Requirement already satisfied: fsspec in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.5.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: multiprocess in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from datasets) (0.70.14)\n","Requirement already satisfied: pandas in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from datasets) (1.5.3)\n","Requirement already satisfied: responses<0.19 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from datasets) (0.18.0)\n","Requirement already satisfied: xxhash in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from datasets) (3.2.0)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from datasets) (12.0.0)\n","Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from datasets) (0.3.6)\n","Requirement already satisfied: aiohttp in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from datasets) (3.8.4)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from aiohttp->datasets) (22.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from aiohttp->datasets) (3.1.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: pytz>=2020.1 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","\u001b[33mWARNING: You are using pip version 21.1.1; however, version 23.1.2 is available.\n","You should consider upgrading via the '/usr/src/python/samuel/fastaiOnCampus/.venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"]}],"source":["! pip install transformers datasets evaluate"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Para realizar o deploy, irei utilizar o *notebook_login* do huggingface para acessar minha conta e poder salvar o modelo.\n","\n","Dessa forma, será feita a instalação do pacote *huggingface_hub* e a utilização do login dele.\n","\n","Ao executar a função *notebook_login()* um prompt aparece com um campo para adicionar um token de acesso disponibilizado pelo huggingface."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-12-14T14:52:45.328749Z","iopub.status.busy":"2022-12-14T14:52:45.328349Z","iopub.status.idle":"2022-12-14T14:52:54.855618Z","shell.execute_reply":"2022-12-14T14:52:54.854321Z","shell.execute_reply.started":"2022-12-14T14:52:45.328707Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: huggingface_hub in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (0.14.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from huggingface_hub) (4.5.0)\n","Requirement already satisfied: requests in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from huggingface_hub) (2.28.2)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from huggingface_hub) (4.65.0)\n","Requirement already satisfied: filelock in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from huggingface_hub) (3.10.7)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from huggingface_hub) (6.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from huggingface_hub) (23.0)\n","Requirement already satisfied: fsspec in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from huggingface_hub) (2023.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from requests->huggingface_hub) (2022.12.7)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from requests->huggingface_hub) (3.1.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from requests->huggingface_hub) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages (from requests->huggingface_hub) (3.4)\n","\u001b[33mWARNING: You are using pip version 21.1.1; however, version 23.1.2 is available.\n","You should consider upgrading via the '/usr/src/python/samuel/fastaiOnCampus/.venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"]}],"source":["! pip install huggingface_hub"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-12-14T14:53:07.930562Z","iopub.status.busy":"2022-12-14T14:53:07.929632Z","iopub.status.idle":"2022-12-14T14:53:08.160647Z","shell.execute_reply":"2022-12-14T14:53:08.159656Z","shell.execute_reply.started":"2022-12-14T14:53:07.930524Z"},"id":"wL_Ptofvk5Li","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a5ef13900148403ca5ad3dee4d78813c","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"]},"metadata":{},"output_type":"display_data"}],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Lb1oELCwk5Li"},"source":["## Preparando Dataset"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["A base de dados utilizada esta disponível no [kaggle](https://www.kaggle.com/datasets/kazanova/sentiment140?resource=download) e possui 1 CSV que contem 1.6 milhão de tweets do twitter. E no intuito de criar um dataset de validação, iremos repartir o dataset em 2 em uma proporção de 80/20. O dataset de treino possui 80% dos dados e o dataset de validação possui 20% dos dados."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-12-14T14:53:21.831724Z","iopub.status.busy":"2022-12-14T14:53:21.831214Z","iopub.status.idle":"2022-12-14T14:53:23.034976Z","shell.execute_reply":"2022-12-14T14:53:23.033910Z","shell.execute_reply.started":"2022-12-14T14:53:21.831667Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","# DATASET\n","DATASET_COLUMNS = [\"labels\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n","DATASET_ENCODING = \"ISO-8859-1\"\n","\n","df = pd.read_csv('./data/training.1600000.processed.noemoticon.csv', encoding=DATASET_ENCODING, names=DATASET_COLUMNS)\n","\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["O conjunto de dados tem 6 campos: \n","- `label`: O sentimento do tweet (0 = negative, 2 = neutral, 4 = positive)\n","- `ids`: O ID do tweet ( 2087)\n","- `date`: A data do tweet (Sat May 16 23:58:44 UTC 2009)\n","- `flag`: A Query (lyx). Se não há query, a seguinte flag está presente NO_QUERY.\n","- `user`: O usuário que twettou (robotickilldozr)\n","- `text`: o texto do tweet."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-12-14T14:53:35.498984Z","iopub.status.busy":"2022-12-14T14:53:35.498537Z","iopub.status.idle":"2022-12-14T14:53:35.524882Z","shell.execute_reply":"2022-12-14T14:53:35.523798Z","shell.execute_reply.started":"2022-12-14T14:53:35.498946Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>labels</th>\n","      <th>ids</th>\n","      <th>date</th>\n","      <th>flag</th>\n","      <th>user</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1467810369</td>\n","      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>_TheSpecialOne_</td>\n","      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>1467810672</td>\n","      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>scotthamilton</td>\n","      <td>is upset that he can't update his Facebook by ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>1467810917</td>\n","      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>mattycus</td>\n","      <td>@Kenichan I dived many times for the ball. Man...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>1467811184</td>\n","      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>ElleCTF</td>\n","      <td>my whole body feels itchy and like its on fire</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>1467811193</td>\n","      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>Karoli</td>\n","      <td>@nationwideclass no, it's not behaving at all....</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1599995</th>\n","      <td>4</td>\n","      <td>2193601966</td>\n","      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>AmandaMarie1028</td>\n","      <td>Just woke up. Having no school is the best fee...</td>\n","    </tr>\n","    <tr>\n","      <th>1599996</th>\n","      <td>4</td>\n","      <td>2193601969</td>\n","      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>TheWDBoards</td>\n","      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n","    </tr>\n","    <tr>\n","      <th>1599997</th>\n","      <td>4</td>\n","      <td>2193601991</td>\n","      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>bpbabe</td>\n","      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n","    </tr>\n","    <tr>\n","      <th>1599998</th>\n","      <td>4</td>\n","      <td>2193602064</td>\n","      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>tinydiamondz</td>\n","      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n","    </tr>\n","    <tr>\n","      <th>1599999</th>\n","      <td>4</td>\n","      <td>2193602129</td>\n","      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n","      <td>NO_QUERY</td>\n","      <td>RyanTrevMorris</td>\n","      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1600000 rows × 6 columns</p>\n","</div>"],"text/plain":["         labels         ids                          date      flag  \\\n","0             0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n","1             0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n","2             0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n","3             0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n","4             0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n","...         ...         ...                           ...       ...   \n","1599995       4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n","1599996       4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n","1599997       4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n","1599998       4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n","1599999       4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n","\n","                    user                                               text  \n","0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n","1          scotthamilton  is upset that he can't update his Facebook by ...  \n","2               mattycus  @Kenichan I dived many times for the ball. Man...  \n","3                ElleCTF    my whole body feels itchy and like its on fire   \n","4                 Karoli  @nationwideclass no, it's not behaving at all....  \n","...                  ...                                                ...  \n","1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n","1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n","1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n","1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n","1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n","\n","[1600000 rows x 6 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["df = df.drop(columns=['date', 'flag', 'user', 'ids'])\n","df['labels'] = df['labels'].apply(lambda number: number/4)\n","df_train, df_valid = np.array_split(df, [int(len(df) * 0.8)])"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>labels</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>is upset that he can't update his Facebook by ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.0</td>\n","      <td>@Kenichan I dived many times for the ball. Man...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.0</td>\n","      <td>my whole body feels itchy and like its on fire</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.0</td>\n","      <td>@nationwideclass no, it's not behaving at all....</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1279995</th>\n","      <td>1.0</td>\n","      <td>@MariaJEchelon  -- 30 Seconds to Mars- The Fan...</td>\n","    </tr>\n","    <tr>\n","      <th>1279996</th>\n","      <td>1.0</td>\n","      <td>@swaffette was a fab party  And the new stuff ...</td>\n","    </tr>\n","    <tr>\n","      <th>1279997</th>\n","      <td>1.0</td>\n","      <td>making the kittens talk in lolspeak.</td>\n","    </tr>\n","    <tr>\n","      <th>1279998</th>\n","      <td>1.0</td>\n","      <td>still aching after yestdy's board sesh.. off t...</td>\n","    </tr>\n","    <tr>\n","      <th>1279999</th>\n","      <td>1.0</td>\n","      <td>@rose_janice ahaha, i love your [finally!]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1280000 rows × 2 columns</p>\n","</div>"],"text/plain":["         labels                                               text\n","0           0.0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n","1           0.0  is upset that he can't update his Facebook by ...\n","2           0.0  @Kenichan I dived many times for the ball. Man...\n","3           0.0    my whole body feels itchy and like its on fire \n","4           0.0  @nationwideclass no, it's not behaving at all....\n","...         ...                                                ...\n","1279995     1.0  @MariaJEchelon  -- 30 Seconds to Mars- The Fan...\n","1279996     1.0  @swaffette was a fab party  And the new stuff ...\n","1279997     1.0             making the kittens talk in lolspeak.  \n","1279998     1.0  still aching after yestdy's board sesh.. off t...\n","1279999     1.0       @rose_janice ahaha, i love your [finally!]  \n","\n","[1280000 rows x 2 columns]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df_train"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-12-14T14:53:39.771277Z","iopub.status.busy":"2022-12-14T14:53:39.770918Z","iopub.status.idle":"2022-12-14T14:53:39.847619Z","shell.execute_reply":"2022-12-14T14:53:39.846739Z","shell.execute_reply.started":"2022-12-14T14:53:39.771245Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>1280000</td>\n","    </tr>\n","    <tr>\n","      <th>unique</th>\n","      <td>1265211</td>\n","    </tr>\n","    <tr>\n","      <th>top</th>\n","      <td>isPlayer Has Died! Sorry</td>\n","    </tr>\n","    <tr>\n","      <th>freq</th>\n","      <td>210</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                             text\n","count                     1280000\n","unique                    1265211\n","top     isPlayer Has Died! Sorry \n","freq                          210"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["df_train.describe(include='object')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Após rodar o comando `describe()` fomos capazes de obter alguns insights interessantes. Um total de 659.775 usuários diferentes realizaram 1.6 milhão de tweets. O usuário 'list_dog' foi o que mais realizou tweets, com um total de 549 publicações."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Iremos então transformar o dataframe em dataset utilizando a lib *datasets* para que ele seja compatível com nosso tokenizer"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-12-14T14:53:43.392506Z","iopub.status.busy":"2022-12-14T14:53:43.392149Z","iopub.status.idle":"2022-12-14T14:53:44.174709Z","shell.execute_reply":"2022-12-14T14:53:44.173641Z","shell.execute_reply.started":"2022-12-14T14:53:43.392476Z"},"trusted":true},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['labels', 'text'],\n","    num_rows: 1280000\n","})"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["from datasets import Dataset, DatasetDict\n","\n","ds_train = Dataset.from_pandas(df_train)\n","ds_train"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"1ELDErRXk5Lk"},"source":["## Pré-processamento do texto (Tokenizando e Numeralização)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["O processo de tokenização é fundamental no NLP. Ele se resume em converter o texto em lista de palavras (podendo ser em caracteres,ou substrings, dependendo da granularidade do modelo utilizado).\n","\n","Após realizar a tokenização, será obtido um vocabulário de palavras únicas presentes no dataset. Como um modelo de deep learning espera números como inputs essas palavras(tokens) são convertidas em números.\n","\n","O modelo utilizado nesse artigo será o *cardiffnlp/twitter-roberta-base-sentiment*.\n","\n","O Twitter-roBERTa-base  base model é uma versão do oriunda do *roBERTa-base model*, treinado com 58 milhoes de tweets. Ele não diferencia caixa alta e caixa baixa, é mais rápido e menor que o BERT."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-12-14T14:53:48.530725Z","iopub.status.busy":"2022-12-14T14:53:48.530154Z","iopub.status.idle":"2022-12-14T14:53:51.490067Z","shell.execute_reply":"2022-12-14T14:53:51.489119Z","shell.execute_reply.started":"2022-12-14T14:53:48.530666Z"},"id":"Jqwv6p-ok5Ll","trusted":true},"outputs":[],"source":["from transformers import AutoTokenizer\n","\n","model_dbu = \"cardiffnlp/twitter-roberta-base-sentiment\"\n","\n","toks = AutoTokenizer.from_pretrained(model_dbu)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"PW0EiSsxk5Ll"},"source":["Função para tokenizar o texto e truncar sequências, se necessário, para que não sejam maiores que o input aceito pelo modelo roBERTa."]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-12-14T14:53:53.911765Z","iopub.status.busy":"2022-12-14T14:53:53.911383Z","iopub.status.idle":"2022-12-14T14:53:53.917004Z","shell.execute_reply":"2022-12-14T14:53:53.915586Z","shell.execute_reply.started":"2022-12-14T14:53:53.911731Z"},"id":"3BuR2kpvk5Ll","trusted":true},"outputs":[],"source":["def tokenizer_function(examples):\n","    return toks(examples[\"text\"], truncation=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_4FvYkBlk5Lm"},"source":["Aplicando a função a todos os elementos do dataset com a função map."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-12-14T14:53:57.120926Z","iopub.status.busy":"2022-12-14T14:53:57.120533Z","iopub.status.idle":"2022-12-14T14:54:26.710853Z","shell.execute_reply":"2022-12-14T14:54:26.709713Z","shell.execute_reply.started":"2022-12-14T14:53:57.120894Z"},"id":"nnbBAfdbk5Lm","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"654d797dd3dc48be84053f50c6334bff","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/1280000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]}],"source":["tokenized_tweets = ds_train.map(tokenizer_function, batched=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Exemplo de tokenização e Numeralização\n","\n","Segue um exemplo de como o *tokenize()* separa o texto em \"tokens\" e como cada um recebe um número no vocabulário."]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-12-14T14:54:30.398384Z","iopub.status.busy":"2022-12-14T14:54:30.397274Z","iopub.status.idle":"2022-12-14T14:54:30.406617Z","shell.execute_reply":"2022-12-14T14:54:30.405723Z","shell.execute_reply.started":"2022-12-14T14:54:30.398342Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['this', 'Ġweek', 'Ġis', 'Ġnot', 'Ġgoing', 'Ġas', 'Ġi', 'Ġhad', 'Ġhoped', 'Ġ']"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["toks.tokenize(\"this week is not going as i had hoped \")"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-12-14T14:54:33.230885Z","iopub.status.busy":"2022-12-14T14:54:33.230510Z","iopub.status.idle":"2022-12-14T14:54:33.252845Z","shell.execute_reply":"2022-12-14T14:54:33.251551Z","shell.execute_reply.started":"2022-12-14T14:54:33.230855Z"},"trusted":true},"outputs":[{"data":{"text/plain":["3583"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["toks.vocab['week']"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Agora observe um dos textos \"tokenizados\" do dataset utilizado"]},{"cell_type":"code","execution_count":14,"metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"execution":{"iopub.execute_input":"2022-12-14T14:54:36.163164Z","iopub.status.busy":"2022-12-14T14:54:36.162808Z","iopub.status.idle":"2022-12-14T14:54:36.174602Z","shell.execute_reply":"2022-12-14T14:54:36.173685Z","shell.execute_reply.started":"2022-12-14T14:54:36.163134Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('this week is not going as i had hoped ',\n"," [0, 9226, 186, 16, 45, 164, 25, 939, 56, 5207, 1437, 2])"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["row = tokenized_tweets[23]\n","row['text'], row['input_ids']"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"FqFeIz8Nk5Lm"},"source":["Agora será criado um lote de exemplos usando o *DataCollatorWithPadding()*, esse lote será utilizado posteriormente como um dos parametrôs do treinamento do modelo."]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-12-14T14:55:20.559530Z","iopub.status.busy":"2022-12-14T14:55:20.559102Z","iopub.status.idle":"2022-12-14T14:55:27.700524Z","shell.execute_reply":"2022-12-14T14:55:27.699252Z","shell.execute_reply.started":"2022-12-14T14:55:20.559488Z"},"id":"TRokgHd4k5Lm","trusted":true},"outputs":[],"source":["from transformers import DataCollatorWithPadding\n","\n","data_collator = DataCollatorWithPadding(tokenizer=toks)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Separando o dataset em teste e treino e validação\n","\n","Para realização do treinamento e testar o modelo após o dataset será dividido em 2 partes, uma contendo 20% dos dados (dados de teste) e a outra com os 80% restantes.\n","\n","Pra isso foi utilizado o método *train_test_split()*."]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-12-14T14:55:41.035191Z","iopub.status.busy":"2022-12-14T14:55:41.034783Z","iopub.status.idle":"2022-12-14T14:55:41.058062Z","shell.execute_reply":"2022-12-14T14:55:41.056915Z","shell.execute_reply.started":"2022-12-14T14:55:41.035149Z"},"trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['labels', 'text', 'input_ids', 'attention_mask'],\n","        num_rows: 1024000\n","    })\n","    test: Dataset({\n","        features: ['labels', 'text', 'input_ids', 'attention_mask'],\n","        num_rows: 256000\n","    })\n","})"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["dds = tokenized_tweets.train_test_split(0.20, seed=42)\n","\n","dds"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['labels', 'input_ids', 'attention_mask'],\n","        num_rows: 1024000\n","    })\n","    test: Dataset({\n","        features: ['labels', 'input_ids', 'attention_mask'],\n","        num_rows: 256000\n","    })\n","})"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["dds = dds.remove_columns('text')\n","dds"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Realizando a limpeza das labels do dataframe de validação, e convertendo em dataset para utilização posterior ao treinamento do modelo.\n","\n","Como pode ser visto o dataset de validação é criado a partir de outro dataframe com dados diferentes do dataframe que gerou o dataset de treino e teste"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-12-14T14:55:44.386817Z","iopub.status.busy":"2022-12-14T14:55:44.385567Z","iopub.status.idle":"2022-12-14T14:55:44.402898Z","shell.execute_reply":"2022-12-14T14:55:44.401958Z","shell.execute_reply.started":"2022-12-14T14:55:44.386766Z"},"trusted":true},"outputs":[],"source":["df_valid.drop(['labels'], axis=1, inplace=True)\n","ds_valid = Dataset.from_pandas(df_valid)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Realizar a separação dos datasets tem um motivo importantissimo.\n","\n","Isso evita que ocorra o **overfitting** e o **underfitting**\n","\n","Uma breve explicação:\n","\n","Um modelo é basicamente um sistema para mapear entradas para saídas. Em suma, um modelo aprende relacionamento entre as entradas (features) e as saídas (labels) a partir de um dataset de treinamento. Depois de treinado o modelo deve ser capaz de receber apenas as entradas (dataset de testes) e ele faz previsões sobre as saídas a partir do que aprendeu no treinamento.\n","\n","O Modelo mais simples é uma regressão linear e para aumentar a complexidade desse modelo basta aumentar o grau polinomial.\n","\n","$$\n","   y = \\beta_0x + \\beta_1 x² = + \\beta_2 x² + ... + \\beta_n x^n + \\epsilon\n","$$\n","\n","![](https://miro.medium.com/max/720/1*pjIp920-MZdS_3fLVhf-Dw.webp)\n","\n","O grau do polinomial é a representação da flexibilida do modelo. E a partir disso podemos entender o que é o undefit e o overfit de um modelo.\n","\n","Um modelo underfit será menos flexível, o que o impede de contabilizar os dados como visto na imagem a seguir.\n","\n","\n","|||\n","|--|--|\n","|![](https://miro.medium.com/max/640/1*kZfqaD6hl9iYGYXkMwV-JA.webp)| ![](https://miro.medium.com/max/640/1*2RXJ2O-_c2ukaq5p-WQ9tQ.webp)\n","\n","A função do modelo na cor laranja, na imagem à esquerda, está acima da função real e do conjunto de dados de treinamento. À direita está a previsão dada pelo modelo. Basicamente o modelo ignora o conjunto de treinamento, pois o modelo underfit tem uma **baixa varância**(depende pouco dos dados de treinamento) e **alto viés** (faz uma forte suposição sobre o comportamento dos dados). Quando o modelo tenta fazer previsões o alto viés o faz ter previsões com alta imprecissão.\n","\n","|||\n","|--|--|\n","|![](https://miro.medium.com/max/640/1*Di7rY6ALXtkhlmlcKRSCoA.webp)| ![](https://miro.medium.com/max/640/1*QzA45ATjeEbwv5f1G99GnQ.webp)\n","\n","Quando o grau do modelo é aumentado muito, o modelo consegue alcançar todas alterações dos dados de treinamento, essa alta flexibilidade  faz com que o modelo tenha uma ótima precisão nos dados de treinamento, mas os dados possuem ruídos e essa **alta flexibilidade** faz com que o modelo se ajuste a eles. Assim o modelo irá possuir uma **alta variância**, ele basicamente **memoriza os dados de treinamento** e não \"aprende\" como é o desejado."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"aBdCJc1lk5Lm"},"source":["## Avaliação\n","\n","Para avaliar o desempenho do modelo é preciso ter uma métrica. \n","\n","Para simplificar a obtenção de um método que possa fazer essa avaliação do modelo, utilizaremos a biblioteca evaluate que possui duzias de métodos de avaliação pra diferentes domínios além do NLP.\n","\n","A métrica que decide por utilizar foi a métrica de *accuracy*"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-12-14T14:59:39.806672Z","iopub.status.busy":"2022-12-14T14:59:39.806093Z","iopub.status.idle":"2022-12-14T14:59:43.047462Z","shell.execute_reply":"2022-12-14T14:59:43.046484Z","shell.execute_reply.started":"2022-12-14T14:59:39.806621Z"},"id":"3cGHVK3gk5Ln","trusted":true},"outputs":[],"source":["import evaluate\n","\n","accuracy = evaluate.load(\"accuracy\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"za4jT4OEk5Ln"},"source":["Essa função utiliza da previsão feita e da label para calcular a métrica utilizada (acurácia)."]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-12-14T14:59:54.318537Z","iopub.status.busy":"2022-12-14T14:59:54.318129Z","iopub.status.idle":"2022-12-14T14:59:54.323931Z","shell.execute_reply":"2022-12-14T14:59:54.322947Z","shell.execute_reply.started":"2022-12-14T14:59:54.318503Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    return accuracy.compute(predictions=predictions, references=labels)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"cUxXN1oak5Ln"},"source":["## Treinamento"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"YXYueR0Nk5Lo"},"source":["Map para converter as labes em palavras significantes e o contrário"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2022-12-14T14:59:57.006319Z","iopub.status.busy":"2022-12-14T14:59:57.005599Z","iopub.status.idle":"2022-12-14T14:59:57.010936Z","shell.execute_reply":"2022-12-14T14:59:57.009978Z","shell.execute_reply.started":"2022-12-14T14:59:57.006283Z"},"trusted":true},"outputs":[],"source":["id_label = {0: \"NEGATIVE\", 0.5: \"NEUTRAL\", 1: 'POSITIVE'}\n","label_id = {\"NEGATIVE\": 0, \"NEUTRAL\": 0.5, 'POSITIVE': 1}"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Carregando o modelo pré treinado roBERTa com o número de labels e o mapeamento delas"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2022-12-14T15:00:21.413137Z","iopub.status.busy":"2022-12-14T15:00:21.412481Z","iopub.status.idle":"2022-12-14T15:00:22.604658Z","shell.execute_reply":"2022-12-14T15:00:22.603665Z","shell.execute_reply.started":"2022-12-14T15:00:21.413095Z"},"id":"YzmNIb_Uk5Lo","trusted":true},"outputs":[],"source":["from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n","\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    model_dbu, \n","    num_labels=3, \n","\n","    id2label=id_label, \n","    label2id=label_id\n",")"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"unsqueeze(): argument 'input' (position 1) must be Tensor, not list","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m torch\u001b[39m.\u001b[39;49munsqueeze([\u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mlabel2\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mlabel3\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39m1\u001b[39;49m)\n","\u001b[0;31mTypeError\u001b[0m: unsqueeze(): argument 'input' (position 1) must be Tensor, not list"]}],"source":["import torch\n","torch.unsqueeze(['label', 'label2', 'label3'], 1)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Instalando as depenências de git para ser possível fazer o push do arquivo do modelo para o HugginFace"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2022-12-14T15:03:48.798906Z","iopub.status.busy":"2022-12-14T15:03:48.797783Z","iopub.status.idle":"2022-12-14T15:03:57.040205Z","shell.execute_reply":"2022-12-14T15:03:57.038890Z","shell.execute_reply.started":"2022-12-14T15:03:48.798849Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","Lendo listas de pacotes... Pronto\n","Construindo árvore de dependências... Pronto\n","Lendo informação de estado... Pronto        \n","git-lfs is already the newest version (3.3.0).\n","0 pacotes atualizados, 0 pacotes novos instalados, 0 a serem removidos e 115 não atualizados.\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","Updated Git hooks.\n","Git LFS initialized.\n"]}],"source":["! sudo apt-get install git-lfs\n","! git lfs install"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Definindo os **argumentos do treinamento com *TrainingArguments()***\n","- `output_dir`: Onde salvar o modelo;\n","- `learning_rate`: taxa de aprendizagem inicial;\n","- `per_device_train_batch_size`: o tamanho da amostra de treino por GPU/TPU;\n","- `per_device_eval_batch_size`: o tamanho da amostra de avaliação por GPU/TPU;\n","- `num_train_epochs`: Número de épocas para o treinamento;\n","- `weight_decay`: A taxa de decaimento do peso a ser aplicada as camadas;\n","- `evaluation_strategy`: A estrátegia de avaliação adotada no treinamento;\n","- `save_strategy`: A estratégia do ponto de salvamento a ser adotada durante o treinamento;\n","- `load_best_model_at_end`: Carregar ou não o melhor modelo encontrado durante o treinamento (Quando verdadeiro exige que os parâmetros \"evaluation_strategy\" e \"save_strategy\" sejam iguais);\n","- `push_to_hub`: \"Push\" do modelo para o Hub, quando ele for salvo (depende do \"output_dir\")."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Conhecendo os parâmetros utilizados para a classe *transofrmers.Trainer*:\n","- `model`: O modelo a ser treinado, avaliado ou utilizado para previsão;\n","- `args`: Os argumentos para ajustar o treinamento;\n","- `train_dataset`: O conjunto de dados que será utilizado para o treinamento,\n","- `eval_dataset`: O conjunto de dados utilizado para avaliação;\n","- `tokenizer`: O tokenizador usado para pré-processar os dados;\n","- `data_collator`: A função utilizada para formar um lote a partir de uma lista de elementos de train_dataset ou eval_dataset;\n","- `compute_metrics`: a função que será utilizada paar computar as métricas na avaliaçã."]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2022-12-14T15:04:43.695820Z","iopub.status.busy":"2022-12-14T15:04:43.694730Z","iopub.status.idle":"2022-12-14T15:06:12.074531Z","shell.execute_reply":"2022-12-14T15:06:12.073246Z","shell.execute_reply.started":"2022-12-14T15:04:43.695773Z"},"id":"4bTKw4gok5Lo","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/src/python/samuel/fastaiOnCampus/nbs/classification_text_model is already a clone of https://huggingface.co/SamuelNog/classification_text_model. Make sure you pull the latest changes with `repo.git_pull()`.\n"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]}],"source":["\n","\n","\n","training_args = TrainingArguments(\n","    output_dir=\"classification_text_model\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=1,\n","    \n","    weight_decay=0.01,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n","    push_to_hub=True,\n","    no_cuda=True,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=dds[\"train\"],\n","    eval_dataset=dds[\"test\"],\n","    tokenizer=toks,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n","    \n",")\n"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2022-12-14T15:06:23.069532Z","iopub.status.busy":"2022-12-14T15:06:23.069145Z","iopub.status.idle":"2022-12-14T15:38:22.311787Z","shell.execute_reply":"2022-12-14T15:38:22.310747Z","shell.execute_reply.started":"2022-12-14T15:06:23.069493Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"ename":"ValueError","evalue":"Target size (torch.Size([16])) must be the same as input size (torch.Size([16, 3]))","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n","File \u001b[0;32m/usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages/transformers/trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1657\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1659\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1661\u001b[0m )\n\u001b[0;32m-> 1662\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1663\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1664\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1665\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1666\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1667\u001b[0m )\n","File \u001b[0;32m/usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages/transformers/trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1927\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1928\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1929\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1931\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1932\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1933\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1934\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1935\u001b[0m ):\n\u001b[1;32m   1936\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1937\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages/transformers/trainer.py:2699\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2696\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2698\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2699\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2701\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2702\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n","File \u001b[0;32m/usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages/transformers/trainer.py:2731\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2729\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2730\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2731\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[1;32m   2732\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2733\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2734\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n","File \u001b[0;32m/usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m/usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py:1253\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1251\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mproblem_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmulti_label_classification\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1252\u001b[0m         loss_fct \u001b[39m=\u001b[39m BCEWithLogitsLoss()\n\u001b[0;32m-> 1253\u001b[0m         loss \u001b[39m=\u001b[39m loss_fct(logits, labels)\n\u001b[1;32m   1255\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1256\u001b[0m     output \u001b[39m=\u001b[39m (logits,) \u001b[39m+\u001b[39m outputs[\u001b[39m2\u001b[39m:]\n","File \u001b[0;32m/usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m/usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages/torch/nn/modules/loss.py:720\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 720\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39;49m, target,\n\u001b[1;32m    721\u001b[0m                                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    722\u001b[0m                                               pos_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_weight,\n\u001b[1;32m    723\u001b[0m                                               reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n","File \u001b[0;32m/usr/src/python/samuel/fastaiOnCampus/.venv/lib/python3.9/site-packages/torch/nn/functional.py:3163\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3160\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (target\u001b[39m.\u001b[39msize() \u001b[39m==\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()):\n\u001b[0;32m-> 3163\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTarget size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) must be the same as input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()))\n\u001b[1;32m   3165\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n","\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([16])) must be the same as input size (torch.Size([16, 3]))"]}],"source":["trainer.train()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["O tempo de treinamento foi longo, apesar de utilizar apenas 2 épocas e realizar o treinamento em uma maquina do kaggle com o uso da GPU T4 X2, o treinamento do modelo demorou cerca de 30 minutos. \n","\n","Assim como configurado nos argumentos o modelo foi salvo em um [repositório](https://huggingface.co/lucasgbezerra/classification_text_model) com o nome especificado na conta logada no ínicio do artigo."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"rU5-H50yk5Lp"},"source":["## Testando o modelo"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Vamos verificar o que o modelo prevê a partir do dataset de testes e validação"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-14T15:43:59.608418Z","iopub.status.busy":"2022-12-14T15:43:59.608031Z","iopub.status.idle":"2022-12-14T15:45:20.503239Z","shell.execute_reply":"2022-12-14T15:45:20.502070Z","shell.execute_reply.started":"2022-12-14T15:43:59.608387Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 8000\n","  Batch size = 32\n","/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/plain":["array([[ 2.23709369, -2.29509497],\n","       [-1.78306019,  1.47043931],\n","       [-1.34510422,  0.93321747],\n","       ...,\n","       [-2.40362573,  1.9160639 ],\n","       [ 0.50562054, -0.47438526],\n","       [-2.07757139,  1.67809868]])"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["preds = trainer.predict(dds['test']).predictions.astype(float)\n","preds"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["A previsão do dataset de testes retornou números menores que -1 e maiores que 1. Para melhorar o entendimento sobre os resultados e definir limites vamos utilizar o método clip do numpy para deixar estes números dentre o range 0 à 1, que nos permite visualizar como porcentagem o grau de positivadade ou negatividade das frases avaliadas."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-14T15:47:12.160631Z","iopub.status.busy":"2022-12-14T15:47:12.159609Z","iopub.status.idle":"2022-12-14T15:47:12.169935Z","shell.execute_reply":"2022-12-14T15:47:12.168756Z","shell.execute_reply.started":"2022-12-14T15:47:12.160573Z"},"id":"9Ok_lUqsk5Lq","trusted":true},"outputs":[{"data":{"text/plain":["array([[1.        , 0.        ],\n","       [0.        , 1.        ],\n","       [0.        , 0.93321747],\n","       ...,\n","       [0.        , 1.        ],\n","       [0.50562054, 0.        ],\n","       [0.        , 1.        ]])"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["preds = np.clip(preds, 0, 1)\n","preds"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Agora utilizando outro conjunto de dados, os dados de validaçaão\n","\n","Primeiro eles serão tokenizados para então poder ocorrer a previsão"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-14T15:47:16.476559Z","iopub.status.busy":"2022-12-14T15:47:16.476205Z","iopub.status.idle":"2022-12-14T15:47:20.219545Z","shell.execute_reply":"2022-12-14T15:47:20.218473Z","shell.execute_reply.started":"2022-12-14T15:47:16.476528Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"83214ca756e542d494639436e5bbaea0","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"}],"source":["toks_valid = ds_valid.map(tokenizer_function, batched=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-14T15:49:24.191328Z","iopub.status.busy":"2022-12-14T15:49:24.190297Z","iopub.status.idle":"2022-12-14T15:50:13.490723Z","shell.execute_reply":"2022-12-14T15:50:13.489719Z","shell.execute_reply.started":"2022-12-14T15:49:24.191278Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 5000\n","  Batch size = 32\n"]},{"data":{"text/plain":["array([[ 1.95127702, -1.99396503],\n","       [ 2.37554598, -2.5228641 ],\n","       [ 2.17845559, -2.19129372],\n","       ...,\n","       [-1.58796299,  1.19799149],\n","       [-2.55518889,  2.01890063],\n","       [-2.28080058,  1.7151109 ]])"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["preds2 = trainer.predict(toks_valid).predictions.astype(float)\n","preds2"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Novamente os resultados serão normalizados entre 0 e 1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-14T15:50:50.652039Z","iopub.status.busy":"2022-12-14T15:50:50.651621Z","iopub.status.idle":"2022-12-14T15:50:50.660157Z","shell.execute_reply":"2022-12-14T15:50:50.659167Z","shell.execute_reply.started":"2022-12-14T15:50:50.651986Z"},"trusted":true},"outputs":[{"data":{"text/plain":["array([[1., 0.],\n","       [1., 0.],\n","       [1., 0.],\n","       ...,\n","       [0., 1.],\n","       [0., 1.],\n","       [0., 1.]])"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["preds2 = np.clip(preds2, 0, 1)\n","preds2"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Resultado do deploy\n","\n","Para realizar o deploy utilizei o modelo salvo no [HuggingFace]().\n","\n","E a partir dele gerei um Space com o seguinte app.py:\n","\n","\n","```python\n","import gradio as gr\n","\n","description = \"Classificador de sentimento (Positivo, Negativo) de textos de avaliações de filmes\"\n","examples = [\n","#             lista de exemplos disponíveis para usar o modelo  \n","        ]\n","\n","gr.Interface.load(\"models/lucasgbezerra/classification_text_model\",  description=description, examples=examples).launch()\n","```\n","\n","Para acessar o deploy basta [clicar aqui]()\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Conclusão\n","\n","Aplicar um modelo pré treinado para NLP é um desafio maior do que o encontrado nos tópicos anteriores de Vision Computer.\n","\n","A utilização das libs do HugginFace facilitaram bastante, principalmente a nível de entendimento.\n","\n","A maior dificuldade, quando comparado a Vision Computer está na preparação dos dados. Dados visuais são muito mais simples de serem separados, e erros mais simples de serem notados no conjunto de dados, quando os dados passam a ser textuais a interpretação deles é mais complexa.\n","\n","Outro ponto a se destacar é o tempo gasto no treinamento, no modelo NLP o tempo gasto em 2 épocas é muito alto, provavelmente muito disso seja devido a quantidade de dados utilizados, cerca de 40 mil textos.\n","\n","Por último, vale destacar que quanto mais se aprofunda nos parâmetros para a construção do modelo, maior o grau de conhecimento se torna necessário. Aspectos como o underfitting e o overfitting se tornam básicos para entender como seu modelo se porta e como melhorá-lo. A métrica de avaliação se torna mais complexa, e pode ser de diversos tipos, cada uma com um foco e se encaixando melhor em determinado modelo."]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat":4,"nbformat_minor":4}
